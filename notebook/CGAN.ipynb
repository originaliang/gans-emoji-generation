{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU_L4IuvqB7H"
   },
   "source": [
    "# Initialization (Enviroments and Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import copy\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "from transformers import BertConfig,BertForSequenceClassification, BertModel, BertTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-trvn8-XXG06"
   },
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "opt = Object()\n",
    "opt.n_epochs = 30000 # number of epochs of training\n",
    "opt.batch_size = 32 # size of the batches\n",
    "opt.lr = 0.0008 # adam: learning rate\n",
    "opt.b1 = 0.7 # adam: decay of first order momentum of gradient\n",
    "opt.b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "opt.n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "opt.latent_dim = 100 # dimensionality of the latent space\n",
    "opt.img_size = 64 # size of each image dimension\n",
    "opt.channels = 3 # number of image channels\n",
    "opt.sample_interval = 1000 # interval between image sampling\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "opt.embedding_dim = 300 # embedding's dimension, BERT:768,  google-word2vec:300\n",
    "opt.reduced_embedding = 100\n",
    "opt_g = copy.copy(opt)\n",
    "opt_d = copy.copy(opt)\n",
    "opt_d.lr = 0.0001\n",
    "opt_d.b1 = 0.4\n",
    "opt_d.b2 = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omcK3iSiL7zX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFbgxgEVIfoU"
   },
   "outputs": [],
   "source": [
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "NLP_MODEL = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                      output_attentions = False, \n",
    "                                      output_hidden_states = True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ROOT_DIR = \"/content/drive/My Drive/emoji_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mId041A8KZrX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ij_JtEuQDwdc"
   },
   "outputs": [],
   "source": [
    "def load_variable(file_name):\n",
    "  with open(ROOT_DIR + \"variables/\" + file_name, \"rb\") as fb:\n",
    "    data = pickle.load(fb)\n",
    "    fb.close()\n",
    "    return data\n",
    "GOOGLE_EMBEDDING = load_variable(\"google_embedding.pkl\")\n",
    "BERT_EMBEDDING = load_variable(\"bert_embedding.pkl\")\n",
    "EMBEDDINGS_LIST = load_variable(\"embeddings_list.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clouOPjGqNpX"
   },
   "source": [
    "# Example of for Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAGqrwzaQ2Zn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "emoji_json = open(ROOT_DIR + \"data.json\")\n",
    "emoji_data = json.load(emoji_json)\n",
    "\n",
    "emoji_data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knHGedoYGe-0"
   },
   "outputs": [],
   "source": [
    "#print(emoji_data)\n",
    "emoji_test = emoji_data['img-facebook-64//1f600.png']\n",
    "print(emoji_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQSTqKY1EfiZ"
   },
   "outputs": [],
   "source": [
    "full_names = []\n",
    "for key,value in emoji_data.items():\n",
    "  emoji = emoji_data[key]\n",
    "  full_names.append(emoji[0]['full_name'])\n",
    "\n",
    "print(full_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJbpN6qzILR0"
   },
   "outputs": [],
   "source": [
    "emoji_info = emoji_data['img-facebook-64//1f600.png']\n",
    "full_name = emoji_info[0]['full_name']\n",
    "print(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kKkHGldLdsR"
   },
   "outputs": [],
   "source": [
    "tokens = TOKENIZER.tokenize(full_name)\n",
    "indexed_tokens = TOKENIZER.convert_tokens_to_ids(tokens)\n",
    "embeddings = TOKENIZER.encode(tokens,\n",
    "                              add_special_tokens = False,\n",
    "                              return_attention_mask = True,   # Construct attn. masks. \n",
    "                              return_tensors = 'pt'   # Return pytorch tensors.\n",
    ")\n",
    "print(embeddings)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAp0zPBQByum"
   },
   "outputs": [],
   "source": [
    "hidden_output = NLP_MODEL(torch.tensor([indexed_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AsehtCCuzoNQ"
   },
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkZAzJDAFvFa"
   },
   "outputs": [],
   "source": [
    "def get_text_embeddings(text, src):\n",
    "  if src == 'google':\n",
    "    sentence_embedding = []\n",
    "    tokens = text.replace(\"-\", \" \")\n",
    "    tokens = tokens.split(' ')\n",
    "    for token in tokens:\n",
    "      if token.lower() not in WORD_VEC.vocab:\n",
    "        #print(token, \" is not in vocab\")\n",
    "        continue\n",
    "      token_embedding = WORD_VEC[token.lower()]\n",
    "      sentence_embedding.append(token_embedding)\n",
    "    #print(np.array(sentence_embedding).shape)\n",
    "    avg_embeddings = np.mean(np.array(sentence_embedding), 0)\n",
    "  elif  src == 'bert':\n",
    "    tokens = TOKENIZER.tokenize(text)\n",
    "    indexed_tokens = TOKENIZER.convert_tokens_to_ids(tokens)\n",
    "    if device.type == 'cuda':\n",
    "      hidden_output = NLP_MODEL(torch.tensor([indexed_tokens]))\n",
    "    else:  \n",
    "      hidden_output = NLP_MODEL(torch.tensor([indexed_tokens]))\n",
    "    embeddings = hidden_output[0][0]\n",
    "    avg_embeddings = torch.mean(embeddings, dim = 0)\n",
    "    return avg_embeddings\n",
    "  elif src == 'google_list':\n",
    "    avg_embeddings = GOOGLE_EMBEDDING[text]\n",
    "  elif src == 'bert_list':\n",
    "    avg_embeddings = BERT_EMBEDDING[text]\n",
    "  return FloatTensor(avg_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPF4emtQVgXy"
   },
   "outputs": [],
   "source": [
    "#test_embedding = get_text_embeddings(\"you do want\", \"google_list\")\n",
    "#print(test_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JC_Wade0HgOv"
   },
   "outputs": [],
   "source": [
    "def get_all_embeddings(emoji_data, root_dir, src):\n",
    "  full_names = []\n",
    "  image_paths = []\n",
    "  embeddings_list = []\n",
    "  for key,value in emoji_data.items():\n",
    "      emoji = emoji_data[key]\n",
    "      full_name = emoji[0][\"full_name\"]\n",
    "      embedding = get_text_embeddings(full_name, src)\n",
    "      embeddings_list.append(embedding.cpu().data.numpy())\n",
    "      full_names.append(full_name)# loaded json data as input\n",
    "      img_path = emoji[0]['path']\n",
    "      image_paths.append( root_dir+ '//'+ img_path)\n",
    "  return full_names, image_paths, np.array(embeddings_list)\n",
    "\n",
    "full_names , image_paths, embeddings_list = get_all_embeddings(emoji_data, ROOT_DIR,'google_list')\n",
    "#embeddings_list = np.transpose(np.array(embeddings_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SqHsdPe_72_"
   },
   "outputs": [],
   "source": [
    "print(embeddings_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-DlE7iTwW14"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings_list(emoji_datesets, src):\n",
    "  saved_embeddings_list = {}\n",
    "\n",
    "  for key, value in emoji_data.items():\n",
    "      emoji = emoji_data[key]\n",
    "      full_name = emoji[0][\"full_name\"]\n",
    "      if full_name not in saved_embeddings_list.keys():\n",
    "         embedding = get_text_embeddings(full_name, src)\n",
    "         saved_embeddings_list[full_name] = embedding.cpu().data.numpy()\n",
    "  return saved_embeddings_list\n",
    "\n",
    "def load_embeddings_list(file_name):\n",
    "   out = []\n",
    "   return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvhYDyyIRkjD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HK8an34qYN8"
   },
   "source": [
    "# Customize Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0L_5u0iXfzCd"
   },
   "outputs": [],
   "source": [
    "# Customize own dataset\n",
    "class EmojiDataset(Dataset):\n",
    "    \"\"\"smiley emoji dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, emoji_data, root_dir=ROOT_DIR, source=\"google_list\", transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (string): Path to the json file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.src = source\n",
    "        self.full_names , self.image_paths, self.embedding_list = get_all_embeddings(emoji_data,root_dir = ROOT_DIR, src= source)\n",
    "        self.emoji_data = emoji_data\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "      \n",
    "        image = cv2.imread(self.image_paths[idx]) / 255 # IO OUTPUT IMAGE WITH 4 CHANNELS\n",
    "\n",
    "        name = self.full_names[idx]\n",
    "\n",
    "        embeddings = get_text_embeddings(name, self.src)\n",
    "\n",
    "        sample = {'image': image, 'embeddings': embeddings}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, embeddings = sample['image'], sample['embeddings']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': Tensor(image),\n",
    "                'embeddings': FloatTensor(embeddings)}\n",
    "                \n",
    "class Rescale(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['embeddings']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "\n",
    "        return {'image': img, 'embeddings': embeddings}\n",
    "\n",
    "emoji_datesets = EmojiDataset(emoji_data, ROOT_DIR, source=\"google_list\", transform=transforms.Compose([\n",
    "                                               ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDp85e4OJ1xg"
   },
   "outputs": [],
   "source": [
    "test = {}\n",
    "'face' in test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eu2X-nT-GqLj"
   },
   "outputs": [],
   "source": [
    "for i in range(len(emoji_datesets)):\n",
    "    sample = emoji_datesets[i]\n",
    "    #print(sample)\n",
    "    print(i, sample['image'].size(), sample['embeddings'].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlDNBtAh_G0x"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(emoji_datesets, batch_size= opt.batch_size ,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhobH31wRlJQ"
   },
   "outputs": [],
   "source": [
    "batch_test = []\n",
    "embedding_test = []\n",
    "for i, sample_batched in enumerate(dataloader):\n",
    "  images_batch, embeddings_batch = sample_batched['image'], sample_batched['embeddings']\n",
    "  print(images_batch.size())\n",
    "  print(embeddings_batch.size())\n",
    "  if i == 1:\n",
    "    batch_test = images_batch\n",
    "    embedding_test = embeddings_batch\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdW6wJKD9IYC"
   },
   "outputs": [],
   "source": [
    "t = embedding_test.unsqueeze(1)\n",
    "t = t.unsqueeze(2)\n",
    "t.repeat(1,4,4,1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nO8KLLPGqrz5"
   },
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Tu2dPdIYnTb"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as func\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_block(in_filters, out_filters, bn=True):\n",
    "            #block = [nn.Conv2d(in_filters, out_filters,3,2,1), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            if bn:\n",
    "                block = [nn.Conv2d(in_filters, out_filters,3, 2, 1, padding_mode=\"replicate\"),\n",
    "                         nn.BatchNorm2d(out_filters, 0.9), nn.LeakyReLU(0.1, inplace=True)]\n",
    "            else:\n",
    "                block = [nn.Conv2d(in_filters, out_filters,3, 2, 1),nn.BatchNorm2d(out_filters, 0.9), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            return block\n",
    "\n",
    "        def linear_block(in_filters, out_filters, activation = None):\n",
    "            block = [nn.Linear(in_filters, out_filters)]\n",
    "            if activation == 'relu':\n",
    "                block.append(nn.LeakyReLU(0.1, inplace = True))\n",
    "            elif activation  == 'sigmoid':\n",
    "                block.append(nn.Sigmoid())\n",
    "            return block\n",
    "        \n",
    "        self.pre_fc = nn.Sequential(nn.Linear(opt.embedding_dim, opt.reduced_embedding),\n",
    "                      nn.BatchNorm1d(opt.reduced_embedding, 0.9), nn.LeakyReLU(0.1, inplace=True))               \n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            *conv_block(opt.channels, 64),\n",
    "            *conv_block(64, 128),\n",
    "            *conv_block(128, 256),\n",
    "            *conv_block(256, 512),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Conv2d(512+ opt.reduced_embedding ,512, 1, 1, padding_mode=\"replicate\"), nn.BatchNorm2d(512, 0.9), nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.fc3 = nn.Sequential(nn.Linear(4 * 4 * 512, 1), nn.Sigmoid()) \n",
    "        # The height and width of downsampled image\n",
    "        #ds_size = opt.img_size // 2 ** 4\n",
    "        #self.adv_layer = nn.Linear(128 * ds_size ** 2, 1)\n",
    "\n",
    "    def forward(self, image, embeddings): \n",
    "        #print('loaded image size: ',image.size())  \n",
    "        image = self.conv_model(image)\n",
    "        # dimension adjustion \n",
    "        embeddings = embeddings.unsqueeze(2)\n",
    "        embeddings = embeddings.unsqueeze(3)\n",
    "        embeddings = embeddings.repeat(1,1,4,4)\n",
    "\n",
    "        cat_input = torch.cat([image, embeddings], 1)\n",
    ")\n",
    "        out = self.fc1(cat_input)\n",
    "\n",
    "        out = out.view(out.shape[0], -1)\n",
    "\n",
    "        out = self.fc3(out)\n",
    "  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgPfaoWqpNkr"
   },
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Un9R928GZtaW"
   },
   "outputs": [],
   "source": [
    "class CGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGenerator, self).__init__()\n",
    "        self.dim = opt.latent_dim\n",
    "\n",
    "        self.linear_block = nn.Sequential(nn.Linear(opt.embedding_dim, opt.reduced_embedding), nn.ReLU())\n",
    "        \n",
    "        self.l1, self.conv_blocks = self.conv_blocks2()\n",
    "          \n",
    "    def forward(self, image, embeddings):\n",
    "        embeddings = self.linear_block(embeddings)\n",
    "        cat_input = torch.cat([image, embeddings], 1)\n",
    "\n",
    "        out = self.l1(cat_input)\n",
    "\n",
    "        out = out.view(out.shape[0], self.layer1_depth, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "\n",
    "        return img\n",
    "   \n",
    "    def conv_blocks2(self):\n",
    "        # ----------------------\n",
    "        # adjustable parameters\n",
    "        self.scale = 0.5\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.layer1_depth = int(1024 * self.scale)\n",
    "        self.layer2_depth = int(512 * self.scale)\n",
    "        self.layer3_depth = int(256 * self.scale)\n",
    "        self.layer4_depth = int(128 * self.scale)\n",
    "        self.layer5_depth = int(64 * self.scale)\n",
    "        # ----------------------\n",
    "\n",
    "        l1 = nn.Sequential(nn.Linear(opt.latent_dim + opt.reduced_embedding, self.layer1_depth * self.init_size ** 2), \n",
    "                           nn.BatchNorm1d(self.layer1_depth * self.init_size ** 2,0.9), nn.ReLU())\n",
    "        # output dim: (self.layer1_depth, self.init_size, self.init_size)\n",
    "        \n",
    "        conv_block = nn.Sequential(\n",
    "            # Layer 1\n",
    "            #nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.ConvTranspose2d(self.layer1_depth, self.layer2_depth, 3, stride=2, padding=1, padding_mode=\"zeros\"),\n",
    "            nn.BatchNorm2d(self.layer2_depth, 0.9),\n",
    "            nn.ReLU(),\n",
    "            # Layer 2        \n",
    "            nn.ConvTranspose2d(self.layer2_depth, self.layer3_depth, 3, stride=2, padding=1, padding_mode=\"zeros\"),\n",
    "            nn.BatchNorm2d(self.layer3_depth, 0.9),\n",
    "            nn.ReLU(),\n",
    "            # output dim: (self.layer3_depth, hh, ww)\n",
    "            # where hh=ww=(h-1)*stride-2*padding+(kernel_size-1)+1\n",
    "\n",
    "            # Layer 3        \n",
    "            nn.Conv2d(self.layer3_depth, self.layer4_depth, 3, stride= 2, padding=1,padding_mode=\"zeros\"),\n",
    "            nn.BatchNorm2d(self.layer4_depth, 0.9),\n",
    "            nn.ReLU(),\n",
    "            # Layer 4            \n",
    "            nn.ConvTranspose2d(self.layer4_depth, opt.channels, 3, stride= 2, padding= 1, padding_mode=\"zeros\"),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        return l1, conv_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SiheWT3vdw-"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOuyOUuPcQWR"
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.05)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPiE64SscRES"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Be1Ly9BQjoxU"
   },
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFpxCWNrcWuh"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "#adversarial_loss = torch.nn.MultiLabelMarginLoss()\n",
    "# dversarial_loss = torch.nn.MSELoss()\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "input = torch.FloatTensor(opt.batch_size, opt.channels*64*64)\n",
    "noise = torch.FloatTensor(opt.batch_size, opt.latent_dim)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = CGenerator()\n",
    "print(generator)\n",
    "discriminator = Discriminator()\n",
    "print(discriminator)\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr1B6PYJcgy9"
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt_g.lr, betas=(opt_g.b1, opt_g.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt_d.lr, betas=(opt_d.b1, opt_d.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvTaR2I3WgHo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "def plot_loss(loss_G, loss_D):\n",
    "    plt.plot(loss_G, 'b',label='Generator Loss ')\n",
    "    plt.plot(loss_D,'r',label='Discriminator Loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.title(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2pcqHmMc9EI"
   },
   "outputs": [],
   "source": [
    "g_loss_list, d_loss_list = [], []\n",
    "#embeddings_list = load_embedding_list()\n",
    "m, n = embeddings_list.shape\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(opt.n_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    gloss = 0\n",
    "    dloss = 0\n",
    "    for i, sample_batched in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        image_batched, embeddings_batched = sample_batched['image'], sample_batched['embeddings']  \n",
    "\n",
    "        valid = Variable(Tensor(image_batched.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(image_batched.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(Tensor(image_batched))\n",
    "        real_embeddings = Variable(FloatTensor(embeddings_batched))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generatorr\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (image_batched.shape[0], opt.latent_dim))))\n",
    "    \n",
    "        choose_idx = np.random.choice(m, image_batched.shape[0])\n",
    "        choose_embeddings = embeddings_list[choose_idx, :]\n",
    "\n",
    "        fake_embeddings = Variable(FloatTensor(choose_embeddings))\n",
    "        #print(z.size(),fake_embeddings.size())\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, real_embeddings)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, real_embeddings)\n",
    "        #g_loss = adversarial_losss(discriminator(gen_img, gen_embeddings), valid)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "        gloss = g_loss.item() + gloss\n",
    "        g_loss.backward()        \n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, real_embeddings)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        #validity_real_fake = discriminator(real_imgs, fake_embeddings)\n",
    "        #d_real_fake_loss = adversarial_loss(validity_real_fake,fake)\n",
    "\n",
    "        validity_fake = discriminator(gen_imgs.detach(), real_embeddings)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "        # Loss for fake images\n",
    "\n",
    "\n",
    "        # Total discriminator loss\n",
    "        #d_loss = d_fake_loss + (d_real_loss + d_real_fake_loss) / 2\n",
    "        d_loss = 0.5*(d_fake_loss + d_real_loss)\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        # real_loss = adversarial_loss(discriminator(real_imgs, real_embeddings), valid)\n",
    "        # fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_embeddings), fake)\n",
    "        # d_loss = 0.5 * (real_loss + fake_loss)\n",
    "        #print(d_fake_loss.item(), d_real_fake_loss.item(), d_loss.item())\n",
    "        dloss = d_loss.item() + dloss\n",
    "        if d_loss.item()/g_loss.item() > 0.1:\n",
    "          d_loss.backward()\n",
    "          optimizer_D.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i + 1\n",
    "        # if batches_done % opt.sample_interval == 0:\n",
    "        #     save_image(gen_imgs.data[:25], \"/content/drive/My Drive/results/2/sigmoid/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        #     # save_image(real_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        \n",
    "        # if batches_done % 10000 == 0:\n",
    "        #     torch.save(generator.state_dict(), \"/content/drive/My Drive/results/2/sigmoid/\" + \"gen_\" + str(batches_done))\n",
    "        #     torch.save(discriminator.state_dict(), \"/content/drive/My Drive/results/2/sigmoid/\" + \"dis_\" + str(batches_done))\n",
    "    g_loss_list.append(gloss)\n",
    "    d_loss_list.append(dloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCg1sKjWoJ8W"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTa03e3gotsi"
   },
   "source": [
    "## Backup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v09m6fP_jGUg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfeLFx-Wpxii"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "WORD_VEC = KeyedVectors.load_word2vec_format(ROOT_DIR + \"google_embedding/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8-H6oa6PiVj"
   },
   "outputs": [],
   "source": [
    "print(WORD_VEC['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4xMtX_2qGF5"
   },
   "outputs": [],
   "source": [
    "google_embeddings_dict = generate_embeddings_list(emoji_data, 'google')\n",
    "bert_embeddings_dict = generate_embeddings_list(emoji_data, 'bert')\n",
    "\n",
    "with open(ROOT_DIR + \"variables/google_embedding.pkl\", 'wb') as fp:\n",
    "    pickle.dump(google_embeddings_dict, fp)\n",
    "\n",
    "with open(ROOT_DIR + \"variables/bert_embedding.pkl\",  'wb') as fp:\n",
    "    pickle.dump(bert_embeddings_dict, fp)\n",
    "\n",
    "with open(ROOT_DIR + \"variables/embeddings_list.pkl\",  'wb') as fp:\n",
    "    pickle.dump(embeddings_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Pg99V1QQOaM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cGAN_BCE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
